# main.py â€”â€” æœ¬åœ° Qwen3-VL + æœ¬åœ° Embedding ç‰ˆæœ¬
# python main.py add_paper papers/Lyapunov-Stable_Deep_Equilibrium_Models.pdf --topics "AI4S,CV,MLLM"
# python main.py add_paper papers/MM-LLMs.pdf --topics "AI4S,CV,MLLM"
# python main.py add_paper papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf --topics "AI4S,CV,MLLM"
# python main.py add_paper papers/Lyapunov-Stable_Deep_Equilibrium_Models.pdf --topics "AI4S,CV,MLLM"
# python main.py add_paper papers/Scientific_discovery_in_the_age_of_artificial_intelligence.pdf --topics "AI4S,CV,MLLM"
# python main.py add_paper papers/Seg-Zero_Reasoning-Chain_Guided_Segmentation_via_Cognitive_Reinforcement.pdf --topics "AI4S,CV,MLLM"
# python main.py search_paper "discrete-time physics"
# python main.py search_paper "Applicable to general energy-based physical models"
# python main.py organize_folder papers --topics "AL4S,CV,MLLM"
#python main.py search_image "æœºæˆ¿"
#python main.py search_image "ä¼é¹…"
#python main.py search_image "è¯ä»¶ç…§"
#python main.py search_image "éŸ³é¢‘"
#python main.py search_image "å¤šæ¨¡æ€"
#rm -rf ./embeddings
import os
import shutil
import argparse
import re
import hashlib
import traceback
from pathlib import Path
from typing import List, Optional

import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
from PyPDF2 import PdfReader
import chromadb
from transformers import (
    Qwen2VLForConditionalGeneration, 
    AutoProcessor, 
    ChineseCLIPProcessor, 
    ChineseCLIPModel,
    logging as hf_logging
)
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForImageTextToText, AutoProcessor # æŠŠ AutoModelForImageTextToText æ¢æˆè¿™ä¸ª
# --- é…ç½® ---
# å‹åˆ¶ Transformers çš„å•°å—¦è­¦å‘Š
hf_logging.set_verbosity_error()

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === æ¨¡å‹è·¯å¾„é…ç½® (è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´) ===
QWEN_MODEL_PATH = "/data0/jycheng/homework/paper_agent/models/Qwen3-VL-2B"
EMBEDDING_MODEL_PATH = "/data0/jycheng/homework/paper_agent/models/bge-m3"
CHINESE_CLIP_PATH = "/data0/jycheng/homework/paper_agent/models/chinese-clip-vit-base-patch16"

# === å…¨å±€å˜é‡ (æ‡’åŠ è½½) ===
_qwen_model = None
_qwen_processor = None
_embedding_model = None
_chroma_client = None
_cclip_model = None
_cclip_processor = None
# 1. ç¡®ä¿å¤´éƒ¨å¯¼å…¥åŒ…å« AutoModelForImageTextToText
from transformers import AutoModelForImageTextToText, AutoProcessor

def get_qwen_model():
    """
    [ä¿®å¤ç‰ˆ] é€‚é… Qwen3-VL
    ä¸å†ç¡¬ç¼–ç  Qwen2 ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨ AutoModel è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç»“æ„
    """
    global _qwen_model, _qwen_processor
    if _qwen_model is not None:
        return _qwen_model, _qwen_processor
    
    print(f"ğŸ§  Loading Qwen3-VL from: {QWEN_MODEL_PATH} ...")
    try:
        # åŠ è½½ Processor (å›¾åƒ/æ–‡æœ¬é¢„å¤„ç†)
        _qwen_processor = AutoProcessor.from_pretrained(QWEN_MODEL_PATH, trust_remote_code=True)
        
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä½¿ç”¨ AutoModelForImageTextToText
        # trust_remote_code=True ä¼šå…è®¸ Qwen3 æ‰§è¡Œå®ƒæ–‡ä»¶å¤¹é‡Œçš„ python ä»£ç æ¥å®šä¹‰å®ƒè‡ªå·±
        _qwen_model = AutoModelForImageTextToText.from_pretrained(
            QWEN_MODEL_PATH,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True 
        ).eval()
        
        print("âœ… Qwen3-VL Loaded Successfully!")
        
    except Exception as e:
        print(f"âš ï¸ Qwen Load Failed: {e}")
        # å¦‚æœ AutoModel ä¹ŸæŒ‚äº†ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯æ ˆä»¥ä¾¿è°ƒè¯•
        traceback.print_exc()
        return None, None
        
    return _qwen_model, _qwen_processor
# -----------------------------------------------------------------------------
# 1. æ¨¡å‹åŠ è½½å‡½æ•° (Lazy Loading)
# -----------------------------------------------------------------------------

def get_chroma_client():
    global _chroma_client
    if _chroma_client is None:
        _chroma_client = chromadb.PersistentClient(path="./embeddings")
    return _chroma_client

def get_embedding_model():
    """åŠ è½½ BGE-M3 (ç”¨äºè®ºæ–‡æ–‡æœ¬æ£€ç´¢)"""
    global _embedding_model
    if _embedding_model is None:
        print("ğŸ”¤ Loading BGE-M3 embedding model...")
        _embedding_model = SentenceTransformer(EMBEDDING_MODEL_PATH, device=DEVICE)
    return _embedding_model



def get_chinese_clip():
    """åŠ è½½ Chinese-CLIP (ç”¨äºå›¾ç‰‡æœç´¢ï¼ŒAll-in-One)"""
    global _cclip_model, _cclip_processor
    if _cclip_model is None:
        print("ğŸ‡¨ğŸ‡³ Loading Chinese-CLIP (All-in-One)...")
        try:
            _cclip_model = ChineseCLIPModel.from_pretrained(CHINESE_CLIP_PATH).to(DEVICE).eval()
            _cclip_processor = ChineseCLIPProcessor.from_pretrained(CHINESE_CLIP_PATH)
        except Exception as e:
            print(f"âŒ Chinese-CLIP Load Failed: {e}")
            raise e
    return _cclip_model, _cclip_processor

# -----------------------------------------------------------------------------
# 2. è¾…åŠ©å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def clean_thinking_content(raw_text: str) -> str:
    """æ¸…æ´— Qwen çš„æ€ç»´é“¾è¾“å‡ºï¼Œåªä¿ç•™æœ€ç»ˆç­”æ¡ˆ"""
    if not raw_text: return ""
    # ç§»é™¤ <think> æ ‡ç­¾
    clean_text = re.sub(r'<think>.*?</think>', '', raw_text, flags=re.DOTALL)
    # ç§»é™¤å¸¸è§å‰ç¼€
    if "Answer:" in clean_text: clean_text = clean_text.split("Answer:")[-1]
    elif "Category:" in clean_text: clean_text = clean_text.split("Category:")[-1]
    # ç§»é™¤ Markdown å’Œæ ‡ç‚¹
    clean_text = clean_text.replace("*", "").replace("`", "").replace('"', "").replace("'", "").strip()
    return clean_text

def extract_pdf_text(pdf_path: str) -> str:
    reader = PdfReader(pdf_path)
    return "\n".join(page.extract_text() or "" for page in reader.pages)
def compute_clip_embedding(text=None, image=None):
    """
    [ç»ˆæä¿®å¤ç‰ˆ] Chinese-CLIP ç»Ÿä¸€å‘é‡è®¡ç®—æ¥å£
    ä¿®å¤ IndexError: tuple index out of range
    """
    model, processor = get_chinese_clip()
    
    with torch.no_grad():
        if image is not None:
            # === å›¾ç‰‡ç¼–ç  (ä¿æŒä¸å˜) ===
            inputs = processor(images=image, return_tensors="pt").to(DEVICE)
            feats = model.get_image_features(**inputs)
            
        elif text is not None:
            # === æ–‡æœ¬ç¼–ç  (æ‰‹åŠ¨æå– [CLS] Token) ===
            inputs = processor(text=[text], return_tensors="pt", padding=True, truncation=True).to(DEVICE)
            
            # 1. è°ƒç”¨åº•å±‚ text_model
            text_outputs = model.text_model(**inputs)
            
            # 2. å®‰å…¨è·å– last_hidden_state
            # æœ‰äº›ç‰ˆæœ¬æ˜¯å¯¹è±¡ï¼Œæœ‰äº›æ˜¯å…ƒç»„ï¼Œè¿™é‡Œåšä¸ªåŒé‡ä¿é™©
            if hasattr(text_outputs, "last_hidden_state"):
                last_hidden_state = text_outputs.last_hidden_state
            else:
                last_hidden_state = text_outputs[0]
            
            # 3. æå– [CLS] Token (å³åºåˆ—çš„ç¬¬ä¸€ä¸ª token)
            # Shape: [batch_size, seq_len, hidden_dim] -> [batch_size, hidden_dim]
            pooled_output = last_hidden_state[:, 0, :]
            
            # 4. æŠ•å½±åˆ° CLIP ç©ºé—´
            feats = model.text_projection(pooled_output)
            
        else:
            return None
        
        # L2 å½’ä¸€åŒ– (CLIP å¿…éœ€)
        feats = feats / feats.norm(p=2, dim=-1, keepdim=True)
        return feats[0].cpu().numpy().tolist()

# -----------------------------------------------------------------------------
# 3. æ ¸å¿ƒåŠŸèƒ½ï¼šè®ºæ–‡ç®¡ç† (Qwen + BGE)
# -----------------------------------------------------------------------------
def classify_paper_with_qwen(text: str, topics: List[str]) -> str:
    """
    [ä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ BGE-M3 è¿›è¡ŒåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†ç±» (æ¯” 2B å¤§æ¨¡å‹æ›´å‡†ã€æ›´å¿«)
    """
    # 1. è·å– Embedding æ¨¡å‹
    emb_model = get_embedding_model()
    
    # 2. æˆªå–æ‘˜è¦ (å‰ 1000 ä¸ªå­—ç¬¦è¶³å¤Ÿåˆ¤æ–­ç±»åˆ«äº†)
    abstract = text[:1000]
    
    print(f"ğŸ§  Classifying via Embedding Similarity...")
    
    # 3. è®¡ç®—ç›¸ä¼¼åº¦
    # ç¼–ç  "ç±»åˆ«è¯" (å¦‚ AI4S, CV, MLLM)
    topic_embeddings = emb_model.encode(topics, normalize_embeddings=True)
    # ç¼–ç  "è®ºæ–‡æ‘˜è¦"
    paper_embedding = emb_model.encode(abstract, normalize_embeddings=True)
    
    # 4. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (ç‚¹ç§¯)
    # paper_embedding @ topic_embeddings.T
    similarities = np.dot(topic_embeddings, paper_embedding)
    
    # 5. æ‰¾å‡ºåˆ†æœ€é«˜çš„é‚£ä¸ª
    best_idx = np.argmax(similarities)
    best_topic = topics[best_idx]
    best_score = similarities[best_idx]
    
    # æ‰“å°ä¸€ä¸‹å…·ä½“çš„å¾—åˆ†ï¼Œè®©ä½ çœ‹åˆ°å®ƒä¸ºä»€ä¹ˆé€‰è¿™ä¸ª
    # æ–¹ä¾¿ä½ è°ƒè¯•ï¼šå¦‚æœ CV çš„åˆ†ä¹Ÿå¾ˆé«˜ï¼Œè¯´æ˜åˆ†ç±»æœ¬èº«æœ‰é‡å 
    score_debug = ", ".join([f"{t}: {s:.2f}" for t, s in zip(topics, similarities)])
    print(f"ğŸ“Š Scores: [{score_debug}]")
    print(f"âœ… Picked: {best_topic}")
    
    return best_topic
def add_paper(pdf_path: str, topics: List[str]):
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        print(f"âŒ File not found: {pdf_path}")
        return

    # 0. æŸ¥é‡é€»è¾‘
    client = get_chroma_client()
    collection = client.get_or_create_collection(name="papers")
    existing = collection.get(where={"filename": pdf_path.name}, limit=1)
    if existing['ids']:
        print(f"â© Skipped (Already indexed): {pdf_path.name}")
        return

    print(f"ğŸ“„ Processing: {pdf_path.name}")
    
    # === 1. è¯»å– PDF å¹¶æå–æ–‡æœ¬ç”¨äºåˆ†ç±» ===
    try:
        reader = PdfReader(str(pdf_path))
        # æå–å‰ä¸¤é¡µçš„å†…å®¹ç”¨äºåˆ†ç±»å°±å¤Ÿäº† (é€šå¸¸æ‘˜è¦åœ¨ç¬¬ä¸€é¡µ)
        full_text_for_classify = ""
        for i in range(min(2, len(reader.pages))):
            full_text_for_classify += reader.pages[i].extract_text() or ""
    except Exception as e:
        print(f"âŒ PDF Error: {e}")
        return

    # === 2. åˆ†ç±» (ä½¿ç”¨ Embedding å¿«é€Ÿåˆ†ç±») ===
    print("ğŸ§  Classifying...")
    topic = classify_paper_with_qwen(full_text_for_classify, topics)
    print(f"âœ… Classified as: {topic}")

    # === 3. ç§»åŠ¨æ–‡ä»¶ ===
    target_dir = Path("organized_papers") / topic
    target_dir.mkdir(parents=True, exist_ok=True)
    dest = target_dir / pdf_path.name
    if not dest.exists():
        shutil.copy(pdf_path, dest)
    print(f"âœ… Saved to: {dest}")

    # === 4. æŒ‰é¡µåˆ‡ç‰‡å¹¶å»ºç«‹ç´¢å¼• (ä¿ç•™é¡µç ä¿¡æ¯) ===
    print("âœ‚ï¸  Chunking & Indexing per page...")
    emb_model = get_embedding_model()
    
    CHUNK_SIZE = 500
    OVERLAP = 50
    file_hash = hashlib.md5(str(dest.resolve()).encode('utf-8')).hexdigest()
    
    batch_chunks = []
    batch_ids = []
    batch_metadatas = []

    # éå†æ¯ä¸€é¡µ
    for page_idx, page in enumerate(reader.pages):
        page_text = page.extract_text()
        if not page_text: continue
        
        # æ¸…æ´—ä¸€ä¸‹ï¼Œå»æ‰å¤šä½™æ¢è¡Œï¼Œæ–¹ä¾¿é˜…è¯»
        page_text = page_text.replace('\n', ' ')
        
        # åœ¨å½“å‰é¡µå†…è¿›è¡Œåˆ‡ç‰‡
        for i in range(0, len(page_text), CHUNK_SIZE - OVERLAP):
            chunk = page_text[i : i + CHUNK_SIZE]
            if len(chunk) < 50: continue # å¤ªçŸ­çš„å¿½ç•¥
            
            chunk_id = f"{file_hash}_p{page_idx+1}_{i}" # IDé‡Œä¹ŸåŠ ä¸Šé¡µç 
            
            batch_chunks.append(chunk)
            batch_ids.append(chunk_id)
            # ã€å…³é”®ã€‘è¿™é‡Œè®°å½• page å­—æ®µ
            batch_metadatas.append({
                "filename": pdf_path.name,
                "topic": topic,
                "path": str(dest),
                "page": page_idx + 1, # è®°å½•é¡µç  (ä»1å¼€å§‹)
                "chunk_index": i
            })

    # æ‰¹é‡å…¥åº“
    if batch_chunks:
        # åˆ†æ‰¹ embedding é˜²æ­¢çˆ†æ˜¾å­˜
        batch_size = 32
        for i in range(0, len(batch_chunks), batch_size):
            end = i + batch_size
            sub_chunks = batch_chunks[i:end]
            sub_ids = batch_ids[i:end]
            sub_metas = batch_metadatas[i:end]
            
            embeddings = emb_model.encode(sub_chunks, normalize_embeddings=True).tolist()
            collection.add(embeddings=embeddings, documents=sub_chunks, metadatas=sub_metas, ids=sub_ids)
            
        print(f"âœ… Indexed {len(batch_chunks)} chunks from {len(reader.pages)} pages.")

def organize_folder(folder_path: str, topics: List[str]):
    source_dir = Path(folder_path)
    pdfs = list(source_dir.glob("**/*.pdf"))
    print(f"ğŸ“‚ Found {len(pdfs)} PDFs in {folder_path}")
    for pdf in pdfs:
        try:
            add_paper(str(pdf), topics)
        except Exception as e:
            print(f"âŒ Skip {pdf.name}: {e}")
def search_paper(query: str, top_k: int = 5, simple: bool = False): # <--- æ”¹åŠ¨1: å¢åŠ  simple å‚æ•°
    emb_model = get_embedding_model()
    query_emb = emb_model.encode([query], normalize_embeddings=True)[0].tolist()
    
    client = get_chroma_client()
    collection = client.get_or_create_collection(name="papers")
    
    # å¦‚æœæ˜¯ç®€æ´æ¨¡å¼ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³å¤šè¦æŠŠä¸€ç‚¹å€™é€‰ï¼Œç„¶åå»é‡
    n_results = top_k * 3 if simple else top_k
    results = collection.query(query_embeddings=[query_emb], n_results=n_results)
    
    print(f"\nğŸ” Results for: '{query}'\n" + "="*60)
    if not results['ids'][0]:
        print("No matches.")
        return

    # === æ–°å¢ï¼šç®€æ´åˆ—è¡¨æ¨¡å¼ ===
    if simple:
        seen_files = set() # ç”¨äºå»é‡
        print("ğŸ“‚ Relevant Files List (Unique):")
        
        for i in range(len(results['ids'][0])):
            meta = results['metadatas'][0][i]
            filename = meta['filename']
            
            # å¦‚æœè¿™ä¸ªæ–‡ä»¶ä¹‹å‰æ²¡å‡ºç°è¿‡ï¼Œå°±æ‰“å°
            if filename not in seen_files:
                print(f"ğŸ“„ {filename}")
                print(f"   path: {meta['path']}")
                seen_files.add(filename)
                
        print("="*60)
        return
    # ==========================

    # åŸæœ‰çš„è¯¦ç»†æ¨¡å¼
    for i in range(len(results['ids'][0])):
        meta = results['metadatas'][0][i]
        content = results['documents'][0][i]
        page_num = meta.get('page', 'Unknown')
        
        print(f"ğŸ“„ File: {meta['filename']}")
        print(f"ğŸ·ï¸  Topic: {meta['topic']}")
        print(f"ğŸ“– Page:  {page_num}")
        print(f"ğŸ“ Context: ...{content}...")
        print("-" * 60)
# -----------------------------------------------------------------------------
# 4. æ ¸å¿ƒåŠŸèƒ½ï¼šå›¾ç‰‡æœç´¢ (Chinese-CLIP All-in-One)
# -----------------------------------------------------------------------------

def add_images_clip(image_dir: str = "images"):
    client = get_chroma_client()
    collection = client.get_or_create_collection(name="images_chinese_clip")
    
    # é¢„åŠ è½½æ¨¡å‹æ£€æŸ¥
    get_chinese_clip()
    
    img_dir_path = Path(image_dir)
    if not img_dir_path.exists():
        print(f"âŒ Folder not found: {image_dir}")
        return

    image_paths = [p for p in img_dir_path.glob("*") if p.suffix.lower() in [".jpg", ".jpeg", ".png", ".webp"]]
    new_count = 0

    for img_path in image_paths:
        # 1. MD5 å»é‡
        path_str = str(img_path.resolve())
        img_id = hashlib.md5(path_str.encode('utf-8')).hexdigest()
        
        try:
            if collection.get(ids=[img_id])['ids']: continue
        except: pass
        
        # 2. ç¼–ç å…¥åº“
        try:
            print(f"âš¡ Indexing: {img_path.name}...")
            image_obj = Image.open(img_path).convert("RGB")
            emb = compute_clip_embedding(image=image_obj)
            
            collection.add(
                embeddings=[emb],
                metadatas=[{"path": str(img_path), "method": "chinese_clip"}],
                ids=[img_id]
            )
            new_count += 1
        except Exception as e:
            print(f"âŒ Error {img_path.name}: {e}")

    if new_count > 0: print(f"âœ… Added {new_count} new images.")
    else: print("âœ… Image index up-to-date.")

def search_image(query: str, top_k: int = 3):
    print(f"ğŸ‡¨ğŸ‡³ Searching with Chinese-CLIP for: '{query}'")
    add_images_clip() # è‡ªåŠ¨æ›´æ–°ç´¢å¼•
    
    query_emb = compute_clip_embedding(text=query)
    
    client = get_chroma_client()
    collection = client.get_or_create_collection(name="images_chinese_clip")
    results = collection.query(query_embeddings=[query_emb], n_results=top_k)
    
    print(f"\nğŸ–¼ï¸  Results:\n" + "="*60)
    if not results['ids'][0]:
        print("No images found.")
        return

    for i in range(len(results['ids'][0])):
        meta = results['metadatas'][0][i]
        distance = results['distances'][0][i] if 'distances' in results else None
        
        # å°†æ¬§æ°è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0-1 ä¹‹é—´)
        # ChromaDB é»˜è®¤ä½¿ç”¨æ¬§æ°è·ç¦»ï¼Œè·ç¦»è¶Šå°è¶Šç›¸ä¼¼
        # ç›¸ä¼¼åº¦ = 1 / (1 + distance)
        similarity = 1 / (1 + distance) if distance is not None else 0.0
        
        print(f"{i+1}. {os.path.basename(meta['path'])}")
        print(f"   Path: {meta['path']}")
        print(f"   â­ Similarity: {similarity:.2%}")
        print()
    print("="*60)
# -----------------------------------------------------------------------------
# 5. CLI ä¸»ç¨‹åº
# -----------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="AI Paper & Image Agent")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # 1. Add Paper
    p_add = subparsers.add_parser("add_paper")
    p_add.add_argument("path", type=str)
    p_add.add_argument("--topics", type=str, default="AI_for_Science,CV,MLLM")

    # 2. Organize Folder
    p_org = subparsers.add_parser("organize_folder")
    p_org.add_argument("folder_path", type=str)
    p_org.add_argument("--topics", type=str, default="AI_for_Science,CV,MLLM")

    # 3. Search Paper
    p_search_p = subparsers.add_parser("search_paper")
    p_search_p.add_argument("query", type=str)
    # æ–°å¢å¼€å…³ï¼Œä¸å¸¦è¿™ä¸ªå‚æ•°å°±æ˜¯ Falseï¼Œå¸¦äº†å°±æ˜¯ True
    p_search_p.add_argument("--simple", action="store_true", help="Only list filenames")

    # 4. Search Image
    p_search_i = subparsers.add_parser("search_image")
    p_search_i.add_argument("query", type=str)

    args = parser.parse_args()

    if args.command == "add_paper":
        topics = [t.strip() for t in args.topics.split(",")]
        add_paper(args.path, topics)
    elif args.command == "organize_folder":
        topics = [t.strip() for t in args.topics.split(",")]
        organize_folder(args.folder_path, topics)
    elif args.command == "search_paper":
        # ä¼ é€’ simple å‚æ•°
        search_paper(args.query, simple=args.simple)
    elif args.command == "search_image":
        search_image(args.query)

if __name__ == "__main__":
    main()